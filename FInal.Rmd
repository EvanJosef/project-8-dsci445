---
title: "Final"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(tidyverse)
library(leaps)
library(ISLR)
library(tree)
library(caret)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
df <- read.csv("OlympicsData/athlete_events.csv")
avg_height <- round(mean(na.omit(df$Height)), 0) 
avg_weight <- round(mean(na.omit(df$Weight)), 0) 
avg_age <- round(mean(na.omit(df$Age)), 0)
df <- df %>%
  replace_na(list(Medal = "No Medal", 
                    Height = avg_height, 
                      Weight = avg_weight, 
                        Age = avg_age)) %>%
  mutate(ID = NULL, NOC = NULL, Games = NULL)
head(df)
```

Due to the Russian doping scandal in the 2012 Olympics 43 Russian athletes were disqualified from the London Olympics. This resulted in 13 medals removed from the Russian slate. The Russian flag/name was also disqualified from the 2020 Olympics, meaning any athletes slated to compete had to representing a different name. Because of this Russia wasn't able to compete in the 2020 olympics, instead a lot of their athletes competed under the name ROC. We will look at the 2020 olympics if the Russian athletes never were caught in the doping scandal by using data from the 2012 olympics. We will use a model selection in order to select the best parts of the dataset to predict a medal and see if disqualifing 'Russia' changed much about the 2020 Tokyo olympics. 

First, we are going to want to split our data into training and testing datasets using medals as the response and all the other variables as the predictors. This will give us a graph that shows us how many polynomials we need. 

There are 498 unique Countries that have won a medal from 1892-2012. We are mostly interested in the last few olympic games as those would be the only athletes slated to also compete in the following few olympics (2016 & 2020/21).  

Look at Bobsleigh explicitly. 

```{r data}
df <- read.csv("OlympicsData/athlete_events.csv")

df <- df %>%
  replace_na(list(Medal = "No Medal", 
                    Height = round(mean(na.omit(df$Height)), 0), 
                      Weight = round(mean(na.omit(df$Weight)), 0), 
                        Age = round(mean(na.omit(df$Age)), 0))) %>%
  mutate(ID = NULL, Games = NULL, Weight = Weight * 2.2046226218, Medal = factor(Medal))

mens <- df[df$Event == "Bobsleigh Men's Two",]

twoman <- df[df$Event == "Bobsleigh Men's Two" | df$Event == "Bobsleigh Women's Two",]
temptwoman <- twoman

binary <- twoman %>%
  mutate(Medal = factor(ifelse(Medal == "No Medal" , 0, 1)))


#twoman$Medal <- ifelse(twoman$Medal == "No Medal", 0, 
#                      ifelse(twoman$Medal == "Gold", 3,
#                              ifelse(twoman$Medal == "Silver", 2,
#                                     ifelse(twoman$Medal == "Bronze", 1, -1))))
fourman <- df[df$Event == "Bobsleigh Men's Four" | df$Event == "Bobsleigh Women's Four",]
fourman$Medal <- ifelse(fourman$Medal == "No Medal", 0, 
                       ifelse(fourman$Medal == "Gold", 3,
                              ifelse(fourman$Medal == "Silver", 2,
                                     ifelse(fourman$Medal == "Bronze", 1, -1))))

```
 
```{r twoman tree}
dftree <- tree(Medal ~ ., data = twoman); summary(dftree)
# Gives us Weight, Height, Age as best predictors

plot(dftree); text(dftree)

ggplot(twoman) +
  geom_point(aes(Weight, Age, colour = temptwoman$Medal)) + 
  geom_vline(xintercept = 240.304, colour = "red") +
  geom_hline(yintercept = 26.5, colour = "red")

n <- nrow(twoman)
trn <- sample(seq_len(n), .75*n)
training <- twoman[twoman$Year <= 2010,]
testing <- twoman[twoman$Year == 2014, ]
#training <- twoman[trn,]
#testing <- twoman[-trn,]

tree_fit <- tree(Medal ~ ., training)
confusion <- table(pred = predict(tree_fit, testing, type = "class"), true = testing$Medal)
(confusion[1, 2] + confusion[2, 1])/sum(confusion)
confusion

#err <- rep(NA, 1000)
#for(i in 1:1000) {
#  trn <- sample(seq_len(n), .75*n)
#  suppressWarnings(tree_fit <- tree(Medal ~., twoman[trn,]))
#  confusion <- table(pred = predict(tree_fit, twoman[-trn,], type = "class"), true = #twoman[-trn,]$Medal)
#  err[i] <- (confusion[1, 2] + confusion[2, 1])/sum(confusion)
#}

#ggplot() + 
#  geom_histogram(aes(err))

```

```{r bagging}
library(randomForest)
bag_fit <- randomForest(Medal ~ ., data = training, mtry = ncol(training) - 1, importance = TRUE)

data.frame(bag_fit$importance )%>%
  mutate(variable = rownames(bag_fit$importance)) %>%
  mutate(variable = factor(variable, levels = variable[order(MeanDecreaseGini)])) %>% 
  ggplot() +
  geom_point(aes(MeanDecreaseGini, variable))

confusion_bag <- table(pred = predict(bag_fit, testing, type = "class"), true = testing$Medal)

(confusion_bag[1, 2] + confusion_bag[2, 1])/sum(confusion_bag)

confusion_bag

varImpPlot(bag_fit)
```

```{r Random Forest}
rf_fit <- randomForest(Medal ~ ., data = training, mtry = sqrt(ncol(training) - 1), importance = TRUE)

data.frame(rf_fit$importance)%>%
  mutate(variable = rownames(rf_fit$importance)) %>%
  mutate(variable = factor(variable, levels = variable[order(MeanDecreaseGini)])) %>% 
  ggplot() +
  geom_point(aes(MeanDecreaseGini, variable))

confusion_rf <- table(pred = predict(rf_fit, testing, type = "class"), true = testing$Medal)

confusion_rf

varImpPlot(rf_fit)
```

```{r binary}
dftree <- tree(Medal ~ Weight + Height + Age, data = binary); summary(dftree)
# Gives us Weight, Height, Age as best predictors

plot(dftree); text(dftree)

ggplot(binary) +
  geom_point(aes(Weight, Age, colour = binary$Medal)) + 
  geom_vline(xintercept = 240.304, colour = "red") +
  geom_hline(yintercept = 26.5, colour = "red")

n <- nrow(binary)
trn <- seq_len(n) %in% sample(seq_len(n), .8*n)
training <- binary[binary$Year <= 2010,]
testing <- binary[binary$Year == 2014,]
#training <- binary[trn, ]
#testing <- binary[!trn, ]

tree_fit <- tree(Medal ~ ., training)
confusion <- table(pred = predict(tree_fit, testing, type = "class"), true = testing$Medal)
(confusion[1, 2] + confusion[2, 1])/sum(confusion)
confusion
```

```{r binary bagging}
bag_fit <- randomForest(Medal ~ Name + Weight + Team + Age + Height, data = training, mtry = ncol(training) - 1, importance = TRUE)
summary(bag_fit)

confusion_bag <- table(pred = predict(bag_fit, testing, type = "class"), true = testing$Medal)

(confusion_bag[1, 2] + confusion_bag[2, 1])/sum(confusion_bag)

confusion_bag
varImpPlot(bag_fit)
```

```{r Random Forest Binary}
rf_fit <- randomForest(Medal ~ ., data = training, mtry = sqrt(ncol(training) - 1), importance = TRUE)
summary(rf_fit)

data.frame(rf_fit$importance)%>%
  mutate(variable = rownames(rf_fit$importance)) %>%
  mutate(variable = factor(variable, levels = variable[order(MeanDecreaseGini)])) %>% 
  ggplot() +
  geom_point(aes(MeanDecreaseGini, variable))

confusion_rf <- table(pred = predict(rf_fit, testing, type = "class"), true = testing$Medal)

(confusion_rf[1, 2] + confusion_rf[2, 1])/sum(confusion_rf)

confusion_rf
```

```{r mens}
dftree <- tree(Medal ~ ., data = mens); summary(dftree)
# Gives us Weight, Height, Age as best predictors

plot(dftree); text(dftree)

ggplot(mens) +
  geom_point(aes(Weight, Age, colour = mens$Medal))

n <- nrow(mens)
training <- mens[mens$Year <= 2010, ]
testing <- mens[mens$Year == 2014, ]

tree_fit <- tree(Medal ~ ., training)
confusion <- table(pred = predict(tree_fit, testing, type = "class"), true = testing$Medal)
(confusion[1, 2] + confusion[2, 1])/sum(confusion)
confusion
```

```{r bagging mens}
bag_fit <- randomForest(Medal ~ ., data = training, mtry = ncol(training) - 1, importance = TRUE)

confusion_bag <- table(pred = predict(bag_fit, testing, type = "class"), true = testing$Medal)

(confusion_bag[1, 2] + confusion_bag[2, 1])/sum(confusion_bag)

bag_fit$confusion
confusion_bag

varImpPlot(bag_fit)
```

```{r Random Forest mens}
rf_fit <- randomForest(Medal ~ ., data = training, mtry = sqrt(ncol(training) - 1), importance = TRUE)

confusion_rf <- table(pred = predict(rf_fit, testing, type = "class"), true = testing$Medal)

confusion_rf
varImpPlot(rf_fit)
```

```{r mens boosting}
library(gbm) 

training <- training %>%
  mutate(Medal = as.numeric(Medal) - 1) %>%
  
boost_fit <- gbm(training$Medal ~ training$Age + training$Weight + training$Height, training, n.trees = 5000, shrinkage = 0.1, interaction.depth = 2)

# 2. Estimate the test error rate using your boosted tree model and compare to all previously fit models.
confusion_boost <- table(pred = predict(boost_fit, testing, type = "response") > 0.5, true = testing$Medal)

## test error rate
(confusion_boost[1, 2] + confusion_boost[2, 1])/sum(confusion_boost)

confusion_boost

## misclassification by true label
c(confusion_boost[2, 1], confusion_boost[1, 2])/colSums(confusion_boost)
```




```{r fourman}
fourman <- df[df$Event == "Bobsleigh Men's Four" | df$Event == "Bobsleigh Women's Four",]
fourman$Medal <- ifelse(fourman$Medal == "No Medal", 0, 
                       ifelse(fourman$Medal == "Gold", 3,
                              ifelse(fourman$Medal == "Silver", 2,
                                     ifelse(fourman$Medal == "Bronze", 1, -1))))

```









```{r}
knn3(Medal ~., data = trianing)
```



